{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2cf3c4",
   "metadata": {},
   "source": [
    "\n",
    "# Predictive Analysis Using Machine Learning — Regression\n",
    "\n",
    "**Goal:** Build and evaluate a machine learning regression model to predict outcomes from a dataset, demonstrating **feature selection**, **model training**, and **evaluation**.\n",
    "\n",
    "**Dataset:** California Housing — available via scikit-learn.\n",
    "\n",
    "**Outline:**\n",
    "1. Load and explore data\n",
    "2. Split data; create preprocessing & baseline model\n",
    "3. Perform feature selection\n",
    "4. Train tuned models\n",
    "5. Evaluate with metrics & visualizations\n",
    "6. Inspect feature importance and conclude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002f1a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/YOGESHWARI/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "X.head(), y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f3822",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/YOGESHWARI/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc3cd3",
   "metadata": {},
   "source": [
    "\n",
    "## Baseline Model (No Feature Selection)\n",
    "\n",
    "We'll start with a linear regression model on all features to establish a baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27caf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline: Linear Regression on all features\n",
    "numeric_features = X.columns.tolist()\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[(\"num\", StandardScaler(), numeric_features)],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "baseline_reg = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "baseline_reg.fit(X_train, y_train)\n",
    "y_pred_base = baseline_reg.predict(X_test)\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_base),\n",
    "    \"RMSE\": mean_squared_error(y_test, y_pred_base, squared=False),\n",
    "    \"R2\": r2_score(y_test, y_pred_base),\n",
    "}\n",
    "baseline_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a81dea",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Selection\n",
    "\n",
    "We'll use **f_regression** with `SelectKBest` to score features and choose the most informative ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try several k values and pick the one with best cross-validated score\n",
    "k_values = [3, 5, 6, 8, X_train.shape[1]]\n",
    "\n",
    "pipe_fs = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"select\", SelectKBest(score_func=f_regression)),\n",
    "    (\"reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\"select__k\": k_values}\n",
    "grid_fs = GridSearchCV(pipe_fs, param_grid=param_grid, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "grid_fs.fit(X_train, y_train)\n",
    "\n",
    "best_k = grid_fs.best_params_[\"select__k\"]\n",
    "best_k, grid_fs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e1b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the best feature selection pipeline on the full training set and evaluate\n",
    "best_fs_model = grid_fs.best_estimator_\n",
    "y_pred_fs = best_fs_model.predict(X_test)\n",
    "\n",
    "fs_metrics = {\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_fs),\n",
    "    \"RMSE\": mean_squared_error(y_test, y_pred_fs, squared=False),\n",
    "    \"R2\": r2_score(y_test, y_pred_fs),\n",
    "}\n",
    "fs_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d124132",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training with an Alternative Regressor\n",
    "\n",
    "We'll also train a **RandomForestRegressor** behind the same preprocessing + feature selection block and tune a couple of key hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    (\"prep\", preprocess),\n",
    "    (\"select\", SelectKBest(score_func=f_regression, k=best_k)),\n",
    "    (\"rf\", RandomForestRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"rf__n_estimators\": [100, 300],\n",
    "    \"rf__max_depth\": [None, 10, 20],\n",
    "    \"rf__min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=5, scoring=\"r2\", n_jobs=-1)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = grid_rf.best_estimator_.predict(X_test)\n",
    "\n",
    "rf_metrics = {\n",
    "    \"MAE\": mean_absolute_error(y_test, y_pred_rf),\n",
    "    \"RMSE\": mean_squared_error(y_test, y_pred_rf, squared=False),\n",
    "    \"R2\": r2_score(y_test, y_pred_rf),\n",
    "}\n",
    "\n",
    "grid_rf.best_params_, rf_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61e978",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Scores and Importances\n",
    "\n",
    "We'll inspect which features were selected and their scores. For tree-based models, we'll also inspect feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2713f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get scores from SelectKBest fitted inside the best linear regression pipeline\n",
    "selector = SelectKBest(score_func=f_regression, k=best_k)\n",
    "selector.fit(preprocess.fit_transform(X_train), y_train)\n",
    "\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = np.array(numeric_features)[selected_mask]\n",
    "scores = selector.scores_[selected_mask]\n",
    "\n",
    "feat_scores = pd.DataFrame({\"feature\": selected_features, \"f_score\": scores}).sort_values(\"f_score\", ascending=False)\n",
    "feat_scores.head(best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ddc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot feature scores\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.barh(feat_scores[\"feature\"], feat_scores[\"f_score\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"F-score\")\n",
    "plt.title(\"Top Features by F-score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature importances from RandomForest\n",
    "rf_model = grid_rf.best_estimator_.named_steps[\"rf\"]\n",
    "selected_feature_names = selected_features\n",
    "\n",
    "importances = rf_model.feature_importances_\n",
    "imp_df = pd.DataFrame({\"feature\": selected_feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "imp_df.head(best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9125be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot RF feature importances\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.barh(imp_df[\"feature\"], imp_df[\"importance\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dcbfb",
   "metadata": {},
   "source": [
    "\n",
    "## Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f86f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"Model\": \"Baseline LinearRegression (all features)\", **baseline_metrics},\n",
    "    {\"Model\": f\"LinearRegression + SelectKBest(k={best_k})\", **fs_metrics},\n",
    "    {\"Model\": \"RandomForest + SelectKBest(best k)\", **rf_metrics},\n",
    "])\n",
    "results.sort_values(\"R2\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6bc9e",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "- We established a strong baseline with Linear Regression.\n",
    "- Using **feature selection (SelectKBest with f_regression)** helped identify the most informative features and can improve interpretability.\n",
    "- An alternative **Random Forest** model provided a useful comparison; depending on hyperparameters and selected features, it can achieve better R² and lower error metrics.\n",
    "- This notebook demonstrates a full workflow: preprocessing, feature selection, model training, hyperparameter tuning, and evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
